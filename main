import os
import numpy as np
from models import GoogLeNet, AlexNet ,MobileNetV3Large
import torchvision.models as models
import torchvision.transforms as transforms
import torch, torchvision, time, datetime
import torch.nn as nn
from sklearn.metrics import confusion_matrix


# 计算混淆矩阵函数
def cal_cm(y_true, y_pred):
    y_true, y_pred = y_true.to('cpu').detach().numpy(), np.argmax(y_pred.to('cpu').detach().numpy(), axis=1)
    y_true, y_pred = y_true.reshape((-1)), y_pred.reshape((-1))
    cm = confusion_matrix(y_true, y_pred, labels=list(range(CLASS_NUM)))
    return cm


# 预处理 为模型增加鲁棒性
data_transforms = {
    'train': transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop((224, 224)),  # 增加泛化能力
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # 亮度对比度等
        # 数据增强
        transforms.RandomChoice([
            transforms.RandomHorizontalFlip(),  # 水平
            transforms.RandomVerticalFlip(),  # 竖直
            transforms.RandomRotation(degrees=45),  # 旋转
        ]),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}

if __name__ == '__main__':
    BATCH_SIZE, EPOCH, CLASS_NUM = 16, 50, len(os.listdir('train'))
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # # 定义mobilenetv2模型 并使用预训练权重增加训练精度
    model = MobileNetV3Large(num_classes=10)
    # # 重定义网络的最后一层，定义为分类的个数
    # model.classifier[-1] = nn.Linear(model.last_channel, CLASS_NUM)
    model.to(DEVICE)
    model.name = 'mobilenetv2'

    # 定义efficientnet_v2_s模型 并使用预训练权重增加训练精度
    # model = torchvision.models.efficientnet_v2_s(pretrained=True)
    # # 重定义网络的最后一层，定义为分类的个数
    # model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, CLASS_NUM)
    # model.to(DEVICE)
    # model.name = 'efficientnet_v2_s'

    # # 定义GoogleNet模型 并使用预训练权重增加训练精度
    # model = GoogLeNet(aux_logits=True, init_weights=True)
    # # 重定义网络的最后一层，定义为分类的个数
    # # model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, CLASS_NUM)
    # model.to(DEVICE)
    # model.name = 'GoogLeNet'

    # # 定义AlexNet模型 并使用预训练权重增加训练精度
    # model = AlexNet(num_classes=10, init_weights=True)
    # # 重定义网络的最后一层，定义为分类的个数
    # # model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, CLASS_NUM)
    # model.to(DEVICE)
    # model.name = 'AlexNet'

    # # 定义shuffflenet模型 并使用预训练权重增加训练精度
    # model = torchvision.models.shufflenet_v2_x2_0(pretrained=True)
    # # 重定义网络的最后一层，定义为分类的个数
    # # model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, CLASS_NUM)
    # model.to(DEVICE)
    # model.name = 'shuffflenet'

    # 数据生成器的定义
    train_generator = torchvision.datasets.ImageFolder('train', transform=data_transforms['train'])
    train_generator = torch.utils.data.DataLoader(train_generator, BATCH_SIZE, shuffle=True, num_workers=4)
    test_generator = torchvision.datasets.ImageFolder('val', transform=data_transforms['test'])
    test_generator = torch.utils.data.DataLoader(test_generator, BATCH_SIZE, shuffle=False, num_workers=4)

    # 优化器 AdamW
    optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4, weight_decay=5e-4)
    # 调节
    lr_step = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-6, T_max=5)

    # loss 多分类 交叉熵
    loss = torch.nn.CrossEntropyLoss(label_smoothing=0.1).to(DEVICE)

    best_acc = 0
    print('{} begin train on {}!'.format(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), DEVICE))
    with open(f'{model.name}.log', 'w+') as f:
        f.write('loss,test_loss,acc,test_acc')

    for epoch in range(EPOCH):
        model.to(DEVICE)
        model.train()
        train_loss, train_cm = [], np.zeros(shape=(CLASS_NUM, CLASS_NUM))
        begin = time.time()
        for x, y in train_generator:
            x, y = x.to(DEVICE), y.to(DEVICE).long()

            pred = model(x.float())
            l = loss(pred, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()

            train_loss.append(float(l.data))
            train_cm += cal_cm(y, pred)
        train_loss = np.mean(train_loss)
        train_acc = np.diag(train_cm).sum() / (train_cm.sum() + 1e-7)

        test_loss, test_cm = [], np.zeros(shape=(CLASS_NUM, CLASS_NUM))
        model.eval()
        with torch.no_grad():
            for x, y in test_generator:
                x, y = x.to(DEVICE), y.to(DEVICE).long()

                pred = model(x.float())
                l = loss(pred, y)
                test_loss.append(float(l.data))
                test_cm += cal_cm(y, pred)
        test_loss = np.mean(test_loss)
        test_acc = np.diag(test_cm).sum() / (test_cm.sum() + 1e-7)

        if test_acc > best_acc:
            best_acc = test_acc
            model.to('cpu')
            torch.save(model, f'{model.name}.pt')
            print("best model will be saved")
        with open(f'{model.name}.log', 'a+') as f:
            f.write('\n{:.5f},{:.5f},{:.4f},{:.4f}'.format(train_loss, test_loss, train_acc, test_acc))
        print(
            '{} epoch:{}, time:{:.2f}s, train_loss:{:.5f}, test_loss:{:.5f}, train_acc:{:.4f}, test_acc:{:.4f}'.format(
                datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                epoch + 1, time.time() - begin, train_loss, test_loss, train_acc, test_acc
            ))
